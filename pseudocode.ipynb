{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplex_model():\n",
    "    X= tokenizer(p)   #posts\n",
    "    for id in author_ids:       #each author has only one Z,theta and b\n",
    "        hidden_state_1_output=bert(X)   #using pretrained-Bert-model as regressor\n",
    "        Z= multiply(Z,topic.T)+multiply(hidden_state_1_output,topic.T)*(1-beta) +multiply(Z,topic.T)*beta  # moving average  (1-beta)*Zt+beta*Zt-1\n",
    "    \n",
    "        hidden_state_2_output=linear(theta)\n",
    "        hidden_state_2_output=clamp(hidden_state_2_output,-0.5,0.5)\n",
    "        theta=hidden_state_2_output*(1-beta)+theta*beta\n",
    "    \n",
    "        hidden_state_3_output=linear(b)\n",
    "        hidden_state_3_output=clamp(hidden_state_3_output,0,1)\n",
    "        b= hidden_state_3_output*(1-beta)+b*beta\n",
    "\n",
    "\n",
    "        Y=Z*[0.5+theta,0.5-theta].T+b\n",
    "\n",
    "    return Y, Z, theta, b\n",
    "\n",
    "\n",
    "def train():\n",
    "    for epoch in n_epochs:\n",
    "        for (post,id,t) in dataloader:\n",
    "            Y_hat, Z, theta, b =simplex_model(post,id,t)\n",
    "\n",
    "        loss=(Y-Y_hat)**2 + lambd1 * l2_penalty(theta) + lambd2 * l2_penalty(b)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_model():\n",
    "    X= tokenizer(p)  \n",
    "    for id in author_ids:       \n",
    "        hidden_state_1_output=bert(X)   \n",
    "        Z= multiply(Z,topic.T)+multiply(hidden_state_1_output,topic.T)*(1-beta) +multiply(Z,topic.T)*beta \n",
    "    \n",
    "        hidden_state_2_output=linear(theta)\n",
    "        hidden_state_2_output=clamp(hidden_state_2_output,-0.5,0.5)\n",
    "        theta=hidden_state_2_output*(1-beta)+theta*beta\n",
    "    \n",
    "        hidden_state_3_output=linear(b)\n",
    "        hidden_state_3_output=clamp(hidden_state_3_output,0,1)\n",
    "        b= hidden_state_3_output*(1-beta)+b*beta\n",
    "\n",
    "        Y=Y=Z*[0.5+theta,0.5-theta].T+b\n",
    "\n",
    "        return Y, Z, theta, b\n",
    "\n",
    "def train():\n",
    "    for epoch in n_epochs:\n",
    "        for (post,id,t) in dataloader:\n",
    "            Y_hat, Z, theta, b =complex_model(post,id,t)\n",
    "\n",
    "        for y in Y:\n",
    "            miu_z_given_y = sum(Z)/n\n",
    "            sigma_z_given_y = (sum((Z-miu_y)**2)/n-1)\n",
    "\n",
    "        for id in author_ids:\n",
    "            for y in Y_hat:\n",
    "                sigma_z = tensor.append(Z.std()).mean()\n",
    " \n",
    "        sigma_y_hat=y_hat.var()\n",
    "        simga_z_given_y_hat = get_variance(sigma_y_hat,sigma_z,theta)\n",
    "        miu_z_given_y_hat = get_mean(Z,y,theta,b)\n",
    "\n",
    "        loss=nn.KLDivLoss(torch.normal(miu_z_given_y,sigma_z_given_y),torch.normal(miu_z_given_y_hat,simga_z_given_y_hat))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variance():\n",
    "    var_inverse=tensor[[1/sigma_y_hat*(0.5+theta)**2+1/sigma_z , (0.25-theta**2)*1/sigma_y_hat]\n",
    "                       [(0.25-theta**2)*1/sigma_y_hat , 1/sigma_y_hat*(0.5-theta)**2+1/sigma_z]]\n",
    "    variance= torch.inverse(var_inverse)\n",
    "    return variance\n",
    "\n",
    "def get_mean():\n",
    "    miu=mean((y_hat-b)*[0.5+theta,0.5-theta].T+Z)\n",
    "    return miu"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
